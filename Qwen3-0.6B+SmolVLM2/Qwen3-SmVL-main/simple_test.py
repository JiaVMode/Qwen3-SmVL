#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–çš„æ¨¡å‹æµ‹è¯•è„šæœ¬
ä½¿ç”¨ä¸è®­ç»ƒè„šæœ¬ç›¸åŒçš„æ–¹å¼åŠ è½½å’Œæµ‹è¯•æ¨¡å‹
"""

import os
import sys
import torch
from PIL import Image

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from utils import load_processor, load_model


def test_model_with_image(image_path, device="cuda:0"):
    """ä½¿ç”¨ä¸è®­ç»ƒè„šæœ¬ç›¸åŒçš„æ–¹å¼æµ‹è¯•æ¨¡å‹"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•æ¨¡å‹...")
    print(f"å›¾åƒè·¯å¾„: {image_path}")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    try:
        # åŠ è½½å¤„ç†å™¨å’Œæ¨¡å‹ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ–¹å¼ï¼‰
        print("ğŸ“¥ åŠ è½½å¤„ç†å™¨...")
        processor = load_processor()
        print("âœ… å¤„ç†å™¨åŠ è½½æˆåŠŸ")
        
        print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
        model = load_model(device)
        
        # å°è¯•åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
        model_path = "/home/lab/LJ/play/Qwen3-0.6B+SmolVLM2/Qwen3-SmVL-main/model/freeze_llm_vlm_cocoqa"
        if os.path.exists(os.path.join(model_path, "model.safetensors")):
            print("ğŸ“¥ åŠ è½½è®­ç»ƒå¥½çš„æƒé‡...")
            from safetensors import safe_open
            checkpoint_path = os.path.join(model_path, "model.safetensors")
            with safe_open(checkpoint_path, framework="pt", device=device) as f:
                state_dict = {key: f.get_tensor(key) for key in f.keys()}
            
            # åŠ è½½æƒé‡ï¼Œå…è®¸éƒ¨åˆ†ä¸åŒ¹é…
            missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
            if missing_keys:
                print(f"âš ï¸  ç¼ºå¤±çš„é”®: {missing_keys}")
            if unexpected_keys:
                print(f"âš ï¸  æ„å¤–çš„é”®: {unexpected_keys}")
            print("âœ… è®­ç»ƒå¥½çš„æƒé‡åŠ è½½æˆåŠŸ")
        
        model.eval()
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # åŠ è½½å›¾åƒ
        image = Image.open(image_path).convert('RGB')
        print(f"âœ… å›¾åƒåŠ è½½æˆåŠŸï¼Œå°ºå¯¸: {image.size}")
        
        # æ„å»ºæµ‹è¯•é—®é¢˜
        question = "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡ä¸­çš„å†…å®¹ã€‚"
        print(f"ğŸ“ é—®é¢˜: {question}")
        
        # æ„å»ºæ¶ˆæ¯ï¼ˆä¸è®­ç»ƒè„šæœ¬ç›¸åŒçš„æ–¹å¼ï¼‰
        messages = [
            {
                "role": "system",
                "content": "ä½¿ç”¨ä¸­æ–‡å›ç­”æ‰€æœ‰é—®é¢˜ã€‚",
            },
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": question},
                ],
            },
        ]
        
        # åº”ç”¨èŠå¤©æ¨¡æ¿
        texts = processor.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=False,
            enable_thinking=False,
        )
        print("################# è¾“å…¥æ–‡æœ¬ #################")
        print(texts)
        
        # æ„å»ºæ‰¹æ¬¡ï¼ˆä¸è®­ç»ƒè„šæœ¬ç›¸åŒçš„æ–¹å¼ï¼‰
        images = [[image]]
        batch = processor(
            text=[texts],
            images=images,
            max_length=1024,
            return_tensors="pt",
            padding_side="left",
            padding=True,
        ).to(model.device, dtype=torch.bfloat16)
        
        # ç”Ÿæˆå“åº”
        print("ğŸ§ª å¼€å§‹ç”Ÿæˆ...")
        with torch.no_grad():
            generated_ids = model.generate(
                **batch, 
                do_sample=False, 
                max_new_tokens=256
            )
        
        # è§£ç è¾“å‡º
        model_context = processor.batch_decode(
            generated_ids, skip_special_tokens=False
        )
        input_ids_len = batch["input_ids"].shape[1]
        generated_texts = processor.batch_decode(
            generated_ids[:, input_ids_len:], skip_special_tokens=True
        )
        
        print("################# ç”Ÿæˆæ–‡æœ¬ #################")
        print(generated_texts[0])
        print("âœ… æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False
    
    return True


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ç®€åŒ–æ¨¡å‹æµ‹è¯•")
    parser.add_argument("--image", type=str, required=True,
                       help="æµ‹è¯•å›¾åƒè·¯å¾„")
    parser.add_argument("--device", type=str, default="cuda:0",
                       help="è®¾å¤‡ç±»å‹ (cuda:0, cpu)")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    if args.device.startswith("cuda") and not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°CPU")
        args.device = "cpu"
    
    test_model_with_image(args.image, args.device) 